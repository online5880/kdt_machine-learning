# 머신러닝
---
## 머신러닝 용어
- 인공지능(AI) : 컴퓨터에 인간의 지능(직관)을 인공적으로 구현하는 방법론 >> 인간의 지능이 가지는 특성을 구현
  - 인간 지능의 특성
    - 학습 : 직간접적 경험, 훈련에 의해 행동 변화를 일으키는 현상
    - 추리 : 어떤 판단의 전제 -> 새로운 판단
    - 적응 : 환경에 따른 변화
    - 논증 : *증명. 어떤 판단이 있을 때, 그 이유를 설명하는 것
  - 인간 지능의 특성  중 1,2개만 구현이 되어 있어도 인공지능이다.
- 머신러닝(ML) : **학습 과정**을 통해 데이터를 기반으로 패턴을 학습하고, 분류, 예측 등과 같은 작업을 수행할 수 있는 기술이다. 주어진 데이터 $x$에 대해 예측된 결과 $y$를 얻기 위해 함수 $f(x)$를 학습하는 과정
- 딥러닝(DL) : *인공신경망. 인간의 신경망을 모방한 구조를 채택. 은닉층 >= 3개 레이어 이상일 때 딥러닝이라고 한다.

---
## 머신러닝의 분류
### 학습 과정에 따른 분류
1. 지도학습 : 훈련 데이터에 정답(레이블, y)이 포함된 경우
2. 비지도학습 : 훈련 데이터에 정답이 포함되지 않은 경우
3. 강화학습 : 행동 - 보상 관계를 학습하며, 최적의 행동을 학습(예: 알파고, 자율 주행)
4. 준지도학습 : 일부 정답이 있는 데이터를 학습하는 과정

### 역할에 따른 분류
1. 분류(Classification) : 특정 클래스가 있을 때, 주어진 데이터는 어떤 클래스에 속하는가를 예측
   - x : 범주형, 연속형
   - y : 범주형만 가능
     - 분류를 할 때 연속형이라면 범주형으로 변환을 해야한다.
2. 회귀(Regression) : 주어진 데이터를 이용해서, 특정 시점(값)이 주어지는 경우 y는 어느 정도의 값이 나오는지 예측
   - y : 연속형
3. 군집화(Clustering): 어떤 데이터가 주어질 때, 비슷한 특성의 데이터를 그룹화
   - y : 없음
   - 비지도 학습

---
## 머신러닝의 장단점
### 장점
- 데이터의 패턴을 쉽게 찾아낼 수 있다.
- 자동화
- 성능 개선

### 단점
- 편향
- 데이터 수집
- 자원 문제

---
## 머신러닝을 배울 때 필요한 지식
- 통계학
  - 확률, 추론, 표집
- 미적분
  - 편미분
  - 정적분
- 선형대수학(가장 자주 쓴다.)
  - 고차원 연산
  - 행렬 연산
- 이산수학
- 컴퓨터과학
  - 최적화
  - 알고리즘

---
## 머신러닝 라이브러리
- sklearn, Tensorflow, Pytorch, Transformers,  diffusers

---
## 데이터 전처리
머신러닝 모델은 $y=f(x)$ 형태이다.

x,y 데이터의 정의가 필요하다.
- y 데이터 : x 데이터와 어떤 관계(상관관계 등)를 가져야 합니다. 분류 모델의 경우 각 클래스별로 개수가 균일해야한다.
- x 데이터
  - 결측치가 없거나 적어야 한다.
  - 이상치가 없거나 적어야 한다.
  - 가능하면 정규분포를 띠는 것이 좋다.
  - 머신러닝 모델은 0~1 사이의 값으로 x를 제한하는 것이 유리하다. (e.g. 기울기 소멸 문제)
  - x 데이터 사의 공선성이 없어야 한다.
  - x 데이터의 차원이 지나치게 높거나 낮으면 안된다.
    - 데이터에 비해 차원이 높으면 과소적합 가능성
    - 연산량 증가
    - 데이터의 차원이 지나치게 낮으면 과적합이 일어날 수 있다.
    - 단순한 결과가 나올 수 있다.

---
## 데이터 편향
(주로 분류 모델에서) 특정 클래스의 개수가 다른 클래스에 비해서 압도적으로 많은 경우, 머신러닝 모델이 절대다수의 클래스 정보를 지나치게 많이 학습할 수 있다.

예시 : A, B 클래스를 분류하려 할 때, A:B = 9990:10 이면, 이 데이터를 학습한 기계학습 모델이  모든 정답을 A로 예측했다고 하면 정확도는 99.9%로 나타난다.

이 문제를 해결하기 위해서 정확도를 제외한 다른 평가지표가 필요하다.

- 혼돈 행렬(confusion matrix) : 예측값과 실제값을 표로 표현한 것. TP, TN, FP, FN를 표현할 수 있다.
  - TP : 예측(참) = 실제(참)
  - TN : 예측(거짓) = 실제(거짓)
  - FP : 예측(참) != 실제(거짓)
  - FN : 예측(거짓) != 실제(참)

### 모델의 평가 지표
- 정확도 : (TP+TN)/(TP+TN+FP+FN)
- 민감도 : TP/(TP+FN) == 재현율 : TP/(TP+FN)
- 특이도 : TN/(FP+TN)
- 정밀도 : TP/(TP+FP)
- F1 Score : 2*(정밀도+재현율)/(재현율+정밀도)

---
## 모델 분류 - 의사결정나무
### Decision Tree
- 참조 강사님 깃허브
- 특성
  - 간단하다.
  - 전치가 거의 또는 전혀 필요없다.
  - 복잡도가 데이터에 거의 정비례하다.
  - 과적합이 쉽게 일어난다.
  - NP-완전 문제이다.

## 군집화 모델 - GMM
- 참고 강사님 깃허브
- 특성
  - 정규분포에 의존
  - 밀도 높은 곳이 겹치면 보기 불편하다.

---
## 모델 분류 - 회귀 모델
### 선형 회귀
- 참조 강사님 깃허브
- 특성
  - 선형성, 등분산성을 띠는 데이터에만 이용 가능
  - 오차의 독립성
  - 다중공선성이 없어야 한다. (독립 변수들 간에 상관관계가 높으면 안된다.)

---

## 교차검증
- 교차검증 : 데이터의 다양한 부분집합을 사용해서 모델을 테스트하는 교차 검증 기술
- 교차검증 사용 목적
  - 과적합 방지
  - 분석 모델의 일반화

> 분석 모델의 일반화(generalization)는 머신러닝에서 매우 중요한 개념으로, 모델이 새로운, 보지 못한 데이터에 대해 얼마나 잘 성능을 발휘하는지를 의미합니다. 모델이 훈련 데이터에 과적합(overfitting)하지 않고, 실제로 사용할 때에도 잘 작동하는지를 나타내는 것이 바로 일반화 능력입니다.

### k-Fold cv
- k-Fold cv : 데이터를 k개로 나눈 뒤, k-1개 집합을 학습 데이터로, 1개를 테스트 데이터로 활용하는 방법
- 이 때, k개로 나눈 데이터는 k가지 학습/테스트 데이터로 조합할 수 있다.
- 검증 과정은 총 k번 진행된다.
- 장점
  - 같은 데이터 양으로 더욱 일반화된 결과를 확인할 수 있다.
  - 데이터 양을 증강시키는 것과 유사한 효과를 볼 수 있다.
- 단점
  - 학습량이 많아진다.
  - 표집 방법에 따라 편향이 생길 수 있다.

### Stratified k-Fold CV
- Stratified k-Fold CV : k-FOld CV 과정에서, 클래스의 비율을 모집합과 같이 조절
- 장점
  - k-Fold CV에 비해 데이터 쏠림 현상이 적어질 것이다.
- 단점
  - 임의 표집이 아니기 때문에, 학습 과정에 이용자의 의도가 들어갈 수 있다.